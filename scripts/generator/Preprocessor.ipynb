{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor\n",
    "\n",
    "This file does all the non-RNN steps we need it to--the ones we only really have to do once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy.random import shuffle\n",
    "from pickle import load\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Our preprocessing method opens our data file and separates each line into pairs of utterances and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Preprocessing Methods\n",
    "######################\n",
    "\n",
    "##### Load the raw dataset #####\n",
    "#This method opens the raw text file, reads the lines, and closes the file.\n",
    "def load_data(filename):\n",
    "    file = open(filename, mode=\"rt\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "##### Split data into utterance-response pairs #####\n",
    "#This method splits the dataset into lines, and for each line, we create a dictionary.\n",
    "#The dictionary key is the utterance (A), and the value is the response (B)\n",
    "#For the utterance and response, the speech-tag and actual utterance is tab separated.\n",
    "#We add each set of utterance-response pairs to an array called pairs.\n",
    "def split_to_pairs(data):\n",
    "    lines = data.split(\"\\n\")\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(\"\\t\")\n",
    "        utterance = tokens[0] + \"\\t\" + tokens[1]\n",
    "        response = tokens[2] + \"\\t\" + tokens[3]\n",
    "        pairs.append([utterance, response])\n",
    "    return pairs\n",
    "\n",
    "##### Clean the data ######\n",
    "#Optionally, we could make all words lowercase, remove punctuation, etc.\n",
    "#I'm going to just leave the dataset in its native form and see how it does for now.\n",
    "#This method essentially just reorganizes the data into a 2D array, where each row holds:\n",
    "# [utterance, response]\n",
    "def clean_data(pairs):\n",
    "    cleaned_data = list()\n",
    "    for pair in pairs:\n",
    "        clean_pair = list()\n",
    "        for utt in pair:\n",
    "            clean_pair.append(utt)\n",
    "        cleaned_data.append(clean_pair)\n",
    "    return array(cleaned_data)\n",
    "\n",
    "##### Save pairs to file #####\n",
    "def save_pairs(pairs, new_filename):\n",
    "    dump(pairs, open(new_filename, \"wb\"))\n",
    "    print(\"Saved: %s\" % new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: resources/utt-resp.pkl\n",
      "[fp\tand I'm calling from Garland, Texas.] => [b\tYeah,]\n",
      "[co^t\tso. anyway, let me press one.] => [aa\tOkay .]\n",
      "[sd\tand, it was an experience that I won't do again .] => [qw\tHow big a family do you have?]\n",
      "[sd\tWe saw people we hadn't see in a while] => [qy\tDid you have people coming from far away?]\n",
      "[sd(^q)\tand we're going, my gosh.] => [sv\tWell you have]\n",
      "[b\tYeah.] => [sv\tAnd if, they come from far away, they take it more seriously]\n",
      "[aa\tI think you're right.] => [b\tYeah.]\n",
      "[b\tYeah.] => [sd\tMy family's not very big]\n",
      "[qw^d\tYour family's from where?] => [sd\tWell, I have a, a brother lives in Indianapolis, a sister lives in Chicago, and my folks live back in Buffalo, New York.]\n",
      "[ba\tno.] => [sd\tI guess we have reunions about once a year or so.]\n"
     ]
    }
   ],
   "source": [
    "#Run preprocessing\n",
    "filepath = \"../../resources/\"\n",
    "filename = filepath + \"clean_dataset.txt\"\n",
    "data = load_data(filename)\n",
    "pairs = split_to_pairs(data)\n",
    "clean_pairs = clean_data(pairs)\n",
    "save_pairs(clean_pairs, filepath + \"utt-resp.pkl\")\n",
    "\n",
    "#Check our dataset\n",
    "#you should see:\n",
    "# [fp\tand I'm calling from Garland, Texas.] => [b\tYeah,], etc.\n",
    "for i in range(10):\n",
    "     print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets and Split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Load dataset methods\n",
    "######################\n",
    "n = 10\n",
    "\n",
    "def load_sentences(filename):\n",
    "    return load(open(filename, \"rb\"))\n",
    "\n",
    "def save_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, \"wb\"))\n",
    "    print(\"Saved: %s\" % filename)\n",
    "    \n",
    "def split_dataset(dataset, num_sentences):\n",
    "    # Take every nth item from the dataset to test on     \n",
    "    test  = dataset[::n]\n",
    "    train = [item for i, item in enumerate(dataset) if (i) % n != 0]\n",
    "    train = np.asarray(train)\n",
    "    \n",
    "    # Compare Data\n",
    "    print(dataset[:10])\n",
    "    print()\n",
    "    print(test[:10])\n",
    "    print()\n",
    "    print(train[:10]) \n",
    "    print()\n",
    "    print(\"Train entries: \", len(train))\n",
    "    print(\"Test entries: \",  len(test))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw data pairs:  46464\n",
      "[['sd\\tand it was just .' 'sv\\tWell, that remark in itself is a slam .']\n",
      " [\"sv\\tThat's nice.\"\n",
      "  \"qy^g\\tNow, being from Philadelphia, I don't expect your dress code to be quite that relaxed, right?\"]\n",
      " [\"sv\\tIt's just a matter of education, I think.\" 'aa\\tYeah,']\n",
      " [\"sd\\tWell I'm going to try to clean up the house after my two children for about an hour see if we can walk around.\"\n",
      "  'b\\tOkay']\n",
      " ['b\\tUh huh.' 'b\\tbut, yeah.']\n",
      " ['qw\\tis that, what is that,' 'sd\\tPalo Alto.']\n",
      " [\"sd\\tyou could say I'm in Baltimore.\" 'b\\tYeah,']\n",
      " ['bf\\tPart of it is technology, yeah.'\n",
      "  'sv\\tBut when, When you get into trouble like that in a place like Vietnam, you do tend to analyze the problems that you get into,']\n",
      " ['sd\\tIt was wonderful.'\n",
      "  'sv\\tthey can do things to shrimp that no one else can.']\n",
      " ['sd\\tYou know, and once a week we drive up into the mountains, you know, usually once a week, once every other week.'\n",
      "  'ba\\tWow,']]\n",
      "\n",
      "[['sd\\tand it was just .' 'sv\\tWell, that remark in itself is a slam .']\n",
      " ['qy\\thowever the question is is that making the difference.'\n",
      "  \"no\\tI don't know.\"]\n",
      " ['sd\\tand we read in the newspaper, southern part of Texas I guess flooded out,'\n",
      "  'b\\tOh, yeah.']\n",
      " ['aa\\tsure.'\n",
      "  'sv\\tbecause, see, the more credit cards you have, the more people offer them to you.']\n",
      " ['sv\\tand, keep Congress to, two terms of two years a piece'\n",
      "  'b\\tUh huh.']\n",
      " ['sd\\tAnd, a lot of times, what I wind up doing is, picking say an actor or an actress, and seeing like all,'\n",
      "  'b\\tYeah.']\n",
      " ['b^r\\tright.' \"sd\\tbut I'm really grateful that I went to college.\"]\n",
      " ['sd\\tand, you can use all kinds of different, textures of thread.'\n",
      "  'bf\\tSo it has the ability to do just about any type of sewing you want to do.']\n",
      " ['qw\\tWell what have you seen?'\n",
      "  'sd\\tI think the last movie that we went out to see was DANCES WITH WOLVES.']\n",
      " [\"sd\\tand that's the thing I probably like the most about it.\"\n",
      "  'qw\\tHow many stories?']]\n",
      "\n",
      "[[\"sv\\tThat's nice.\"\n",
      "  \"qy^g\\tNow, being from Philadelphia, I don't expect your dress code to be quite that relaxed, right?\"]\n",
      " [\"sv\\tIt's just a matter of education, I think.\" 'aa\\tYeah,']\n",
      " [\"sd\\tWell I'm going to try to clean up the house after my two children for about an hour see if we can walk around.\"\n",
      "  'b\\tOkay']\n",
      " ['b\\tUh huh.' 'b\\tbut, yeah.']\n",
      " ['qw\\tis that, what is that,' 'sd\\tPalo Alto.']\n",
      " [\"sd\\tyou could say I'm in Baltimore.\" 'b\\tYeah,']\n",
      " ['bf\\tPart of it is technology, yeah.'\n",
      "  'sv\\tBut when, When you get into trouble like that in a place like Vietnam, you do tend to analyze the problems that you get into,']\n",
      " ['sd\\tIt was wonderful.'\n",
      "  'sv\\tthey can do things to shrimp that no one else can.']\n",
      " ['sd\\tYou know, and once a week we drive up into the mountains, you know, usually once a week, once every other week.'\n",
      "  'ba\\tWow,']\n",
      " ['sd\\tand now that my husband has retired, I am back substitute teaching,'\n",
      "  'b^r\\tYes.']]\n",
      "\n",
      "Train entries:  27000\n",
      "Test entries:  3000\n",
      "Saved: resources/utt-resp-both.pkl\n",
      "Saved: resources/utt-resp-train.pkl\n",
      "Saved: resources/utt-resp-test.pkl\n"
     ]
    }
   ],
   "source": [
    "#For testing purposes, you can change n_sentences, the number of trained sentences, to a smaller number.\n",
    "raw_dataset = load_sentences(filepath + \"utt-resp.pkl\")\n",
    "print(\"Number of raw data pairs: \", len(raw_dataset))\n",
    "n_sentences = 30000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "shuffle(dataset)\n",
    "train, test = split_dataset(dataset, n_sentences)\n",
    "save_sentences(dataset, filepath + \"utt-resp-both.pkl\")\n",
    "save_sentences(train, filepath + \"utt-resp-train.pkl\")\n",
    "save_sentences(test, filepath + \"utt-resp-test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
